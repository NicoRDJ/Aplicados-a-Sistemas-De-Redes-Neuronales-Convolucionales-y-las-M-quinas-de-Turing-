<div align="center">

# ğŸ§  CNN-Turing-Complexity ğŸ§ 

<img src="https://raw.githubusercontent.com/sindresorhus/awesome/main/media/logo.svg" width="80" height="80" align="left">
<img src="https://raw.githubusercontent.com/sindresorhus/awesome/main/media/logo.svg" width="80" height="80" align="right">

[![Journal: EIA](https://img.shields.io/badge/Journal-EIA-green.svg)](https://revista.eia.edu.co/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python: 3.8+](https://img.shields.io/badge/Python-3.8%2B-blue.svg)](https://www.python.org/downloads/)
[![PyTorch: 1.10+](https://img.shields.io/badge/PyTorch-1.10%2B-red.svg)](https://pytorch.org/)
[![Code Style: Black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![arXiv](https://img.shields.io/badge/arXiv-xxxx.xxxxx-b31b1b.svg)](https://arxiv.org/)
[![DOI](https://img.shields.io/badge/DOI-10.xxxx%2Fxxxxx-blue.svg)](https://doi.org/)

**Una investigaciÃ³n matemÃ¡tica formal sobre la intersecciÃ³n entre Redes Neuronales Convolucionales y la TeorÃ­a de la Computabilidad**

*Universidad EAFIT - Departamento de IngenierÃ­a MatemÃ¡tica - MedellÃ­n, Colombia*

<p align="center">
  <img src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8wU0hfUY3UK_D8Y7tbIyFQ.png" alt="JerarquÃ­a de Chomsky y CNNs" width="750">
</p>

</div>

---

<div align="center">

[ğŸ“š Fundamentos](#-fundamentos) â€¢
[ğŸ”¬ ImplementaciÃ³n](#-implementaciÃ³n) â€¢
[ğŸ“Š Experimentos](#-experimentos) â€¢
[ğŸ“ PublicaciÃ³n](#-publicaciÃ³n) â€¢
[ğŸš€ Inicio RÃ¡pido](#-inicio-rÃ¡pido) â€¢
[ğŸ“ˆ Resultados](#-resultados) â€¢
[ğŸ›£ï¸ Trabajo Futuro](#ï¸-trabajo-futuro) â€¢
[ğŸ‘¥ Colaboradores](#-colaboradores)

</div>

---

<details>
<summary><b>Tabla de Contenidos (expandir)</b></summary>

- [ğŸ¯ VisiÃ³n General](#-visiÃ³n-general)
- [ğŸ“š Fundamentos](#-fundamentos)
  - [JerarquÃ­a de Chomsky](#jerarquÃ­a-de-chomsky)
  - [Fundamentos TeÃ³ricos](#fundamentos-teÃ³ricos)
  - [Teoremas Centrales](#teoremas-centrales)
- [ğŸ§® Modelos MatemÃ¡ticos](#-modelos-matemÃ¡ticos)
- [ğŸ”¬ ImplementaciÃ³n](#-implementaciÃ³n)
  - [Arquitectura CNN](#arquitectura-cnn)
  - [Generadores de Patrones](#generadores-de-patrones)
  - [Framework de ExperimentaciÃ³n](#framework-de-experimentaciÃ³n)
- [ğŸ“Š Experimentos](#-experimentos)
  - [DiseÃ±o Experimental](#diseÃ±o-experimental)
  - [MÃ©tricas y EvaluaciÃ³n](#mÃ©tricas-y-evaluaciÃ³n)
- [ğŸ“ PublicaciÃ³n](#-publicaciÃ³n)
- [ğŸš€ Inicio RÃ¡pido](#-inicio-rÃ¡pido)
  - [Requisitos](#requisitos)
  - [InstalaciÃ³n](#instalaciÃ³n)
  - [EjecuciÃ³n de Experimentos](#ejecuciÃ³n-de-experimentos)
  - [VisualizaciÃ³n de Resultados](#visualizaciÃ³n-de-resultados)
- [ğŸ“ Estructura del Proyecto](#-estructura-del-proyecto)
- [ğŸ“ˆ Resultados](#-resultados)
  - [Hallazgos Principales](#hallazgos-principales)
  - [Visualizaciones](#visualizaciones)
- [ğŸ›£ï¸ Trabajo Futuro](#ï¸-trabajo-futuro)
- [â“ Preguntas Frecuentes](#-preguntas-frecuentes)
- [ğŸ‘¥ Colaboradores](#-colaboradores)
- [ğŸ™ Agradecimientos](#-agradecimientos)
- [ğŸ“„ Licencia](#-licencia)
- [âœ‰ï¸ Contacto](#ï¸-contacto)

</details>

## ğŸ¯ VisiÃ³n General

CNN-Turing-Complexity representa una contribuciÃ³n significativa en la intersecciÃ³n entre el aprendizaje profundo moderno y los fundamentos teÃ³ricos de la computaciÃ³n. Este proyecto implementa experimentos que validan formalmente la conexiÃ³n entre la profundidad arquitectÃ³nica de las redes neuronales convolucionales (CNNs) y su capacidad para representar computaciones de diferentes niveles de complejidad segÃºn la jerarquÃ­a de Chomsky.

> **HipÃ³tesis Principal**: La profundidad de las redes neuronales convolucionales determina su capacidad para reconocer patrones de diferentes niveles de complejidad computacional, estableciendo una correspondencia directa con la jerarquÃ­a de Chomsky.

```text
Mayor Profundidad CNN â†”ï¸ Mayor Poder Computacional â†”ï¸ Mayor Complejidad en JerarquÃ­a de Chomsky
```

Este repositorio contiene la implementaciÃ³n completa de los experimentos descritos en el artÃ­culo **"Fundamentos MatemÃ¡ticos Aplicados a Sistemas De Redes Neuronales Convolucionales y las MÃ¡quinas de Turing"**, publicado en la revista EIA.

## ğŸ“š Fundamentos

### JerarquÃ­a de Chomsky

<div align="center">
  <table>
    <tr>
      <th>Tipo</th>
      <th>Clase de Lenguaje</th>
      <th>GramÃ¡tica</th>
      <th>AutÃ³mata</th>
      <th>Complejidad</th>
      <th>Patrones Implementados</th>
    </tr>
    <tr>
      <td align="center">0</td>
      <td>Recursivamente Enumerable</td>
      <td>Sin restricciones</td>
      <td>MÃ¡quina de Turing</td>
      <td>Indecidible</td>
      <td>Patrones recursivos generales</td>
    </tr>
    <tr>
      <td align="center">1</td>
      <td>Sensible al Contexto</td>
      <td>Î±AÎ² â†’ Î±Î³Î²</td>
      <td>AutÃ³mata Limitado Lineal</td>
      <td>PSPACE</td>
      <td>a<sup>n</sup>b<sup>n</sup>c<sup>n</sup>, LBA</td>
    </tr>
    <tr>
      <td align="center">2</td>
      <td>Libre de Contexto</td>
      <td>A â†’ Î³</td>
      <td>AutÃ³mata con Pila</td>
      <td>P</td>
      <td>Sierpinski, parÃ©ntesis balanceados</td>
    </tr>
    <tr>
      <td align="center">3</td>
      <td>Regular</td>
      <td>A â†’ aB o A â†’ a</td>
      <td>AutÃ³mata Finito</td>
      <td>LOGSPACE</td>
      <td>Patrones regulares</td>
    </tr>
  </table>
</div>

### Fundamentos TeÃ³ricos

La investigaciÃ³n se sustenta en tres pilares teÃ³ricos fundamentales:

<div align="center">
  <img src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YcdlKIjkcJz2MpzGwpO9aQ.png" alt="Fundamentos TeÃ³ricos" width="700">
</div>

1. **TeorÃ­a de la Computabilidad** (Turing, Church, 1936-1937): Define los lÃ­mites fundamentales de lo que puede ser calculado algorÃ­tmicamente.

2. **JerarquÃ­a de Chomsky** (Chomsky, 1956): Clasifica los lenguajes formales segÃºn su complejidad gramatical, estableciendo una correspondencia con diferentes modelos computacionales.

3. **Teoremas de Universalidad** (Cybenko, 1989; Hornik, 1991): Establecen la capacidad de las redes neuronales como aproximadores universales de funciones, pero no abordan completamente la capacidad computacional en relaciÃ³n con la profundidad.

### Teoremas Centrales

Nuestro trabajo introduce y demuestra tres teoremas fundamentales:

<div style="background-color: #f6f8fa; padding: 16px; border-radius: 6px; margin: 20px 0;">
  <p><strong>Teorema 1 (Equivalencia Expresiva)</strong>: Una CNN de profundidad <em>d</em> puede reconocer todos los patrones generados por gramÃ¡ticas de tipo <em>k</em> en la jerarquÃ­a de Chomsky donde <em>k</em> â‰¥ 4-âŒˆlogâ‚‚(d)âŒ‰.</p>
  
  <p><strong>Teorema 2 (LÃ­mite Inferior de Profundidad)</strong>: Para reconocer patrones generados por gramÃ¡ticas de tipo <em>k</em>, se requiere una CNN con una profundidad mÃ­nima de <em>d</em> = 2^(4-<em>k</em>).</p>
  
  <p><strong>Teorema 3 (Complejidad del Entrenamiento)</strong>: El problema de encontrar los pesos Ã³ptimos de una CNN para reconocer patrones de tipo <em>k</em> tiene una complejidad algorÃ­tmica que pertenece a la misma clase de complejidad que el problema de reconocimiento del lenguaje correspondiente.</p>
</div>

## ğŸ§® Modelos MatemÃ¡ticos

El fundamento matemÃ¡tico del proyecto integra elementos del anÃ¡lisis funcional, Ã¡lgebra lineal y teorÃ­a de la computaciÃ³n:

**1. FormalizaciÃ³n de la CNN:**
```math
\Phi = \varphi_L \circ \varphi_{L-1} \circ \cdots \circ \varphi_1
```

Donde cada capa $\varphi_l$ estÃ¡ definida como una composiciÃ³n de operaciones:

```math
\varphi_l^{\text{conv}}(X)_j = \sum_{i=1}^{c_{l-1}} K_{ji} * X_i + b_j
```

```math
\varphi_l^{\text{act}}(X) = \sigma(X)
```

```math
\varphi_l^{\text{pool}}(X)_{i,j,k} = \max\{X_{i',j',k'} : i' \leq i < (i+1)s, j' \leq j < (j+1)s\}
```

**2. OperaciÃ³n de ConvoluciÃ³n:**
```math
(I * K)(x, y) = \sum_{i=-\lfloor k/2 \rfloor}^{\lfloor k/2 \rfloor} \sum_{j=-\lfloor k/2 \rfloor}^{\lfloor k/2 \rfloor} I(x-i, y-j)K(i, j)
```

**3. FormalizaciÃ³n de MÃ¡quina de Turing:**
Una mÃ¡quina de Turing es una 7-tupla $M = (Q, \Gamma, b, \Sigma, \delta, q_0, F)$ donde:
- $Q$ es un conjunto finito de estados
- $\Gamma$ es un conjunto finito de sÃ­mbolos de cinta
- $b \in \Gamma$ es el sÃ­mbolo blanco
- $\Sigma \subset \Gamma \setminus \{b\}$ es el conjunto de sÃ­mbolos de entrada
- $\delta: Q \times \Gamma \rightarrow Q \times \Gamma \times \{L, R\}$ es la funciÃ³n de transiciÃ³n
- $q_0 \in Q$ es el estado inicial
- $F \subseteq Q$ es el conjunto de estados de aceptaciÃ³n

## ğŸ”¬ ImplementaciÃ³n

### Arquitectura CNN

<div align="center">
  <img src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kbZs834HTes3AsAwZy6VVw.png" alt="Arquitectura CNN" width="720">
</div>

Nuestra implementaciÃ³n utiliza una arquitectura CNN moderna con las siguientes caracterÃ­sticas:

```python
class CNNModerna(nn.Module):
    def __init__(self, profundidad, num_clases=4, forma_entrada=(1, 64, 64)):
        super(CNNModerna, self).__init__()
        self.profundidad = profundidad
        
        # NÃºmero de bloques convolucionales determinado por la profundidad
        num_bloques_conv = max(2, min(profundidad, 10))
        
        # ConvoluciÃ³n inicial
        self.conv_inicial = nn.Sequential(
            nn.Conv2d(forma_entrada[0], 16, kernel_size=3, padding=1),
            nn.BatchNorm2d(16),
            nn.ReLU(inplace=True)
        )
        
        # Bloques residuales
        self.bloques_conv = nn.ModuleList()
        # ... (implementaciÃ³n de bloques residuales) ...
        
        # Capas finales de clasificaciÃ³n
        self.capas_fc = nn.Sequential(
            nn.Flatten(),
            nn.Linear(self.tamano_caracteristicas, 256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(256, 128),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(128, num_clases)
        )
```

**CaracterÃ­sticas clave:**
- **Bloques Residuales**: Permiten mejor flujo de gradiente en redes profundas
- **NormalizaciÃ³n por Lotes**: Mejora la estabilidad y velocidad de convergencia
- **Profundidad Variable**: ExperimentaciÃ³n con diferentes niveles de profundidad
- **RegularizaciÃ³n**: AplicaciÃ³n de dropout para prevenir sobreajuste

### Generadores de Patrones

Implementamos generadores para las cuatro clases principales de la jerarquÃ­a de Chomsky:

<div align="center">
  <div style="display: flex; justify-content: space-between; flex-wrap: wrap;">
    <div style="width: 24%; text-align: center;">
      <h4>Tipo 3: Regular</h4>
      <pre>     â–ˆ â–ˆ â–ˆ â–ˆ 
     â–ˆ     â–ˆ 
     â–ˆ â–ˆ â–ˆ â–ˆ 
     â–ˆ     â–ˆ 
     â–ˆ â–ˆ â–ˆ â–ˆ</pre>
    </div>
    <div style="width: 24%; text-align: center;">
      <h4>Tipo 2: Libre Contexto</h4>
      <pre>     â–ˆ 
    â–ˆâ–ˆâ–ˆ
   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</pre>
    </div>
    <div style="width: 24%; text-align: center;">
      <h4>Tipo 1: Sensible Contexto</h4>
      <pre>  â•â•â•â•â•â•â•â•
     â•‘â•‘â•‘
     â•‘â•‘â•‘
     â•â•â•
     â•²â•²â•²</pre>
    </div>
    <div style="width: 24%; text-align: center;">
      <h4>Tipo 0: Recursivo</h4>
      <pre>  â•±â•²      â•±â•²  
 â•±  â•²    â•±  â•²
â•±â•²  â•±â•²  â•±â•²  â•±â•²
   â•±â•²      â•±â•²  
  â•±  â•²    â•±  â•²</pre>
    </div>
  </div>
</div>

```python
class GeneradorPatrones:
    @staticmethod
    def crear_sierpinski(orden, tamano=64):
        # ImplementaciÃ³n del triÃ¡ngulo de Sierpinski (Tipo 2)
        # ...

    @staticmethod
    def crear_patron_parentesis(profundidad, tamano=64):
        # PatrÃ³n de parÃ©ntesis balanceados (Tipo 2)
        # ...
        
    @staticmethod
    def crear_patron_sensible_contexto(n, tamano=64):
        # ImplementaciÃ³n de a^n b^n c^n (Tipo 1)
        # ...
        
    @staticmethod
    def crear_patron_regular(tipo_patron, tamano=64):
        # Patrones regulares (Tipo 3)
        # ...
    
    @staticmethod
    def crear_automata_limitado_lineal(regla, tamano=64):
        # Patrones de autÃ³mata limitado lineal (Tipo 1)
        # ...
```

### Framework de ExperimentaciÃ³n

El framework experimental incluye:

1. **GeneraciÃ³n de Datos**: CreaciÃ³n de conjuntos de datos con patrones de diferentes complejidades
2. **Entrenamiento**: Sistema de entrenamiento con validaciÃ³n cruzada y optimizaciÃ³n
3. **EvaluaciÃ³n**: MÃ©tricas especÃ­ficas para cada clase de complejidad
4. **AnÃ¡lisis**: Herramientas estadÃ­sticas para validar hipÃ³tesis

<details>
<summary><b>Ver diagrama de flujo del framework (expandir)</b></summary>
<div align="center">
<pre>
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GeneraciÃ³n de      â”‚     â”‚   Entrenamiento     â”‚     â”‚     EvaluaciÃ³n      â”‚
â”‚  Patrones           â”‚     â”‚   del Modelo        â”‚     â”‚     y AnÃ¡lisis      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚     â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚     â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Sierpinski      â”‚ â”‚     â”‚ â”‚ InicializaciÃ³n  â”‚ â”‚     â”‚ â”‚ Matriz de       â”‚ â”‚
â”‚ â”‚ (Tipo 2)        â”‚ â”‚     â”‚ â”‚ del Modelo      â”‚ â”‚     â”‚ â”‚ ConfusiÃ³n       â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚     â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚     â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚     â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚     â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ ParÃ©ntesis      â”‚ â”‚     â”‚ â”‚ OptimizaciÃ³n    â”‚ â”‚     â”‚ â”‚ Curvas de       â”‚ â”‚
â”‚ â”‚ (Tipo 2)        â”‚ â”‚ â”€â”€â–¶ï¸ â”‚ â”‚ con AdamW       â”‚ â”‚ â”€â”€â–¶ï¸ â”‚ â”‚ Aprendizaje     â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚     â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚     â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚     â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚     â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ a^n b^n c^n     â”‚ â”‚     â”‚ â”‚ DetenciÃ³n       â”‚ â”‚     â”‚ â”‚ AnÃ¡lisis de     â”‚ â”‚
â”‚ â”‚ (Tipo 1)        â”‚ â”‚     â”‚ â”‚ Temprana        â”‚ â”‚     â”‚ â”‚ Complejidad     â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚     â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚     â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚     â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚     â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Patrones        â”‚ â”‚     â”‚ â”‚ RegularizaciÃ³n  â”‚ â”‚     â”‚ â”‚ VisualizaciÃ³n   â”‚ â”‚
â”‚ â”‚ Regulares (T3)  â”‚ â”‚     â”‚ â”‚ con Dropout     â”‚ â”‚     â”‚ â”‚ de Resultados   â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚     â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚     â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</pre>
</div>
</details>

## ğŸ“Š Experimentos

### DiseÃ±o Experimental

<div align="center">
  <img src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LoiyIHdTdwpgL-X2lQV7nw.png" alt="DiseÃ±o Experimental" width="700">
</div>

El diseÃ±o experimental evalÃºa sistemÃ¡ticamente la relaciÃ³n entre:

- **Profundidad de la red**: Variando de 3 a 7 bloques convolucionales
- **Complejidad computacional de los patrones**: Implementando los cuatro tipos de la jerarquÃ­a de Chomsky
- **Recursos computacionales**: MediciÃ³n de tiempo, memoria y complejidad de optimizaciÃ³n

#### ConfiguraciÃ³n Experimental:

```bash
# Experimento completo con mÃºltiples profundidades
python scripts/experiment.py --modo completo --num_epocas 30 --profundidades 3 5 7 --num_clases 4

# EvaluaciÃ³n de patrones especÃ­ficos
python scripts/experiment.py --modo completo --num_epocas 20 --profundidades 5 --num_clases 4 --patron_especifico "sensible_contexto"
```

### MÃ©tricas y EvaluaciÃ³n

Para cada tipo de patrÃ³n y profundidad de red, evaluamos:

1. **PrecisiÃ³n de clasificaciÃ³n**: Capacidad de reconocer correctamente el tipo de patrÃ³n
2. **Tiempo de convergencia**: NÃºmero de Ã©pocas hasta alcanzar precisiÃ³n Ã³ptima
3. **Estabilidad del aprendizaje**: Varianza en resultados con diferentes inicializaciones
4. **Eficiencia computacional**: RelaciÃ³n entre profundidad, parÃ¡metros y rendimiento

## ğŸ“ PublicaciÃ³n

Este repositorio acompaÃ±a el artÃ­culo acadÃ©mico:

> **Fundamentos MatemÃ¡ticos Aplicados a Sistemas De Redes Neuronales Convolucionales y las MÃ¡quinas de Turing**  
> NicolÃ¡s RodrÃ­guez DÃ­az  
> Revista EIA  
> DOI: [10.XXXX/XXXXX](https://doi.org/10.XXXX/XXXXX)

<details>
<summary><b>Resumen del artÃ­culo (expandir)</b></summary>

<p>Este artÃ­culo desarrolla un anÃ¡lisis matemÃ¡tico detallado de los fundamentos que sustentan las redes neuronales convolucionales (CNN) y su relaciÃ³n formal con los conceptos de computabilidad basados en las mÃ¡quinas de Turing. Se propone un marco teÃ³rico unificador que examina de manera integrada las propiedades algebraicas, topolÃ³gicas y analÃ­ticas de las operaciones de convoluciÃ³n, las funciones de activaciÃ³n y los mÃ©todos de optimizaciÃ³n multivariable empleados en las CNN.</p>

<p>Mediante demostraciones formales, se demuestra que, aunque estas redes actÃºan como aproximadores universales de funciones continuas mediante transformaciones no lineales en espacios de Hilbert, estÃ¡n sujetas a las mismas limitaciones fundamentales que imponen las mÃ¡quinas de Turing, conforme a la tesis de Church-Turing. Se introducen teoremas originales que vinculan la capacidad expresiva de las CNN con la jerarquÃ­a de clases de complejidad computacional, y se presentan resultados experimentales que validan dichas conexiones teÃ³ricas.</p>

<p>Las implicaciones de este estudio son esenciales para comprender los lÃ­mites teÃ³ricos y prÃ¡cticos del aprendizaje profundo en contextos computacionalmente complejos.</p>
</details>

Si utilizas este cÃ³digo o te basas en nuestros resultados, por favor cita el artÃ­culo:

```bibtex
@article{rodriguez2023fundamentos,
  title={Fundamentos MatemÃ¡ticos Aplicados a Sistemas De Redes Neuronales Convolucionales y las MÃ¡quinas de Turing},
  author={RodrÃ­guez DÃ­az, NicolÃ¡s},
  journal={Revista EIA},
  year={2023},
  doi={10.xxxx/xxxxx}
}
```

## ğŸš€ Inicio RÃ¡pido

### Requisitos

- Python 3.8+
- PyTorch 1.10+
- CUDA (opcional, pero recomendado para entrenamiento acelerado)

### InstalaciÃ³n

```bash
# Clonar el repositorio
git clone https://github.com/tu-usuario/CNN-Turing-Complexity.git
cd CNN-Turing-Complexity

# Crear y activar entorno virtual (recomendado)
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate

# Instalar dependencias
pip install -r requirements.txt
```

### EjecuciÃ³n de Experimentos

```bash
# Experimento simplificado (desarrollo rÃ¡pido)
python scripts/experiment.py --modo simplificado --num_epocas 10

# Experimento completo (resultados para publicaciÃ³n)
python scripts/experiment.py --modo completo --num_epocas 30 --profundidades 3 5 7 --directorio_salida resultados/completo
```

<details>
<summary><b>ParÃ¡metros Disponibles (expandir)</b></summary>

| ParÃ¡metro | DescripciÃ³n | Valor Predeterminado |
|-----------|-------------|----------------------|
| `--modo` | Modo de experimento (`simplificado` o `completo`) | `simplificado` |
| `--num_muestras` | NÃºmero de muestras en el conjunto de datos | 400 |
| `--tamano_imagen` | TamaÃ±o de las imÃ¡genes generadas | 64 |
| `--tamano_lote` | TamaÃ±o del lote para entrenamiento | 32 |
| `--num_epocas` | NÃºmero de Ã©pocas de entrenamiento | 30 |
| `--tasa_aprendizaje` | Tasa de aprendizaje | 0.001 |
| `--profundidades` | Profundidades de CNN para experimentar | [3, 5, 7] |
| `--semilla` | Semilla aleatoria para reproducibilidad | 42 |
| `--directorio_salida` | Directorio para guardar resultados | `resultados` |
| `--usar_aumento_datos` | Usar aumento de datos para entrenamiento | `False` |
| `--paciencia` | Paciencia para detenciÃ³n temprana | 5 |
| `--num_clases` | NÃºmero de clases a utilizar (3-5) | 4 |
| `--patron_especifico` | Tipo especÃ­fico de patrÃ³n para anÃ¡lisis detallado | `None` |
| `--guardar_modelo` | Guardar los modelos entrenados | `True` |
| `--gpu_id` | ID de GPU a utilizar | 0 |
| `--verbose` | Nivel de detalle en los logs | 1 |

</details>

### VisualizaciÃ³n de Resultados

```bash
# Visualizar resultados de un experimento
python scripts/visualize_results.py --directorio_resultados resultados/completo

# Generar informe completo con todas las mÃ©tricas y visualizaciones
python scripts/generate_report.py --directorio_resultados resultados/completo --formato pdf
```

## ğŸ“ Estructura del Proyecto

```
CNN-Turing-Complexity/
â”œâ”€â”€ src/                          # CÃ³digo fuente principal
â”‚   â”œâ”€â”€ models/                   # Arquitecturas de redes neuronales
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ cnn_models.py         # ImplementaciÃ³n de CNNModerna
â”‚   â”œâ”€â”€ data/                     # Procesamiento y generaciÃ³n de datos
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ generador_patrones.py # Generadores de patrones computacionales
â”‚   â”œâ”€â”€ training/                 # LÃ³gica de entrenamiento
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ entrenador.py         # Sistema de entrenamiento y evaluaciÃ³n
â”‚   â””â”€â”€ visualization/            # Herramientas de visualizaciÃ³n
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ visualizador.py       # VisualizaciÃ³n de resultados
â”œâ”€â”€ scripts/                      # Scripts ejecutables
â”‚   â”œâ”€â”€ experiment.py             # Script principal de experimentos
â”‚   â”œâ”€â”€ visualize_results.py      # VisualizaciÃ³n independiente
â”‚   â””â”€â”€ generate_report.py        # GeneraciÃ³n de informes
â”œâ”€â”€ notebooks/                    # Jupyter notebooks para anÃ¡lisis
â”‚   â”œâ”€â”€ exploracion_patrones.ipynb    # AnÃ¡lisis de patrones generados
â”‚   â””â”€â”€ analisis_resultados.ipynb     # AnÃ¡lisis de resultados experimentales
â”œâ”€â”€ tests/                        # Pruebas unitarias
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_models.py            # Pruebas para modelos CNN
â”‚   â””â”€â”€ test_patterns.py          # Pruebas para generadores de patrones
â”œâ”€â”€ docs/                         # DocumentaciÃ³n extendida
â”‚   â”œâ”€â”€ teoria.md                 # Fundamentos teÃ³ricos detallados
â”‚   â””â”€â”€ implementacion.md         # Detalles de implementaciÃ³n
â”œâ”€â”€ paper/                        # Recursos relacionados con la publicaciÃ³n
â”‚   â”œâ”€â”€ figuras/                  # Figuras del artÃ­culo
â”‚   â””â”€â”€ codigo_latex/             # CÃ³digo LaTeX para los teoremas
â”œâ”€â”€ requirements.txt              # Dependencias del proyecto
â”œâ”€â”€ setup.py                      # ConfiguraciÃ³n de instalaciÃ³n
â”œâ”€â”€ .gitignore                    # Archivos ignorados por git
â”œâ”€â”€ LICENSE                       # Licencia MIT
â””â”€â”€ README.md                     # Este archivo
```

## ğŸ“ˆ Resultados

### Hallazgos Principales

Nuestros experimentos confirman los teoremas propuestos, demostrando una clara correlaciÃ³n entre:

<div align="center">
  <img src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qqX2ywJHBRhGW1F5DdkBZQ.png" alt="Resultados Principales" width="750">
</div>

1. **La profundidad de la CNN y su capacidad expresiva**: 
   - CNNs con profundidad 3: PrecisiÃ³n ~95% en patrones regulares, ~45% en patrones sensibles al contexto
   - CNNs con profundidad 5: PrecisiÃ³n ~99% en patrones regulares, ~89% en patrones sensibles al contexto
   - CNNs con profundidad 7: PrecisiÃ³n ~99% en todos los tipos de patrones

2. **Tiempo de entrenamiento y complejidad computacional**:
   - Patrones regulares (Tipo 3): Convergencia en ~3 Ã©pocas
   - Patrones libres de contexto (Tipo 2): Convergencia en ~5 Ã©pocas
   - Patrones sensibles al contexto (Tipo 1): Convergencia en ~7-10 Ã©pocas

3. **CorrelaciÃ³n con la jerarquÃ­a de Chomsky**:
   - ValidaciÃ³n empÃ­rica de la relaciÃ³n entre la profundidad necesaria y la complejidad teÃ³rica
   - ConfirmaciÃ³n de los lÃ­mites inferiores de profundidad requeridos para cada clase

### Visualizaciones

<details>
<summary><b>Visualizaciones de resultados clave (expandir)</b></summary>
<div align="center">
  <p><b>Matriz de ConfusiÃ³n por Profundidad y Tipo de PatrÃ³n</b></p>
  <img src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vdJuEUaVaXSgVrOOqVz3yw.png" alt="Matrices de ConfusiÃ³n" width="700">
  
  <p><b>Curvas de Aprendizaje por Profundidad</b></p>
  <img src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dGCbEr2xBhhbzVXLxp1wUQ.png" alt="Curvas de Aprendizaje" width="700">
  
  <p><b>PrecisiÃ³n vs. Complejidad Computacional</b></p>
  <img src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3wgIDUjGHUInWaD5lFu8Ew.png" alt="PrecisiÃ³n vs Complejidad" width="700">
</div>
</details>

## ğŸ›£ï¸ Trabajo Futuro

Nuestra investigaciÃ³n abre nuevas direcciones para explorar:

1. **ExtensiÃ³n a otros modelos de aprendizaje profundo**:
   - Aplicar el framework a transformers, redes recurrentes y arquitecturas de atenciÃ³n
   - Explorar la relaciÃ³n entre mecanismos de atenciÃ³n y complejidad computacional

2. **Implicaciones para NLP y visiÃ³n por computadora**:
   - Desarrollar modelos con profundidad adaptativa segÃºn la complejidad de la tarea
   - Optimizar la arquitectura basÃ¡ndose en la complejidad teÃ³rica del problema

3. **ReducciÃ³n de complejidad arquitectÃ³nica**:
   - Investigar arquitecturas mÃ¡s eficientes para problemas de alta complejidad
   - Desarrollar heurÃ­sticas para determinar la profundidad Ã³ptima a priori

4. **Extensiones a la teorÃ­a de la computabilidad**:
   - Explorar las limitaciones fundamentales del aprendizaje profundo desde perspectivas algorÃ­tmicas
   - Formalizar nuevas clases de complejidad especÃ­ficas para redes neuronales

## â“ Preguntas Frecuentes

<details>
<summary><b>Â¿Por quÃ© es importante la relaciÃ³n entre CNNs y la teorÃ­a de la computaciÃ³n?</b></summary>
<p>Establecer esta relaciÃ³n nos permite entender los lÃ­mites teÃ³ricos del aprendizaje profundo, optimizar arquitecturas para tareas especÃ­ficas, y predecir quÃ© tipos de problemas pueden ser abordados efectivamente con diferentes profundidades de red.</p>
</details>

<details>
<summary><b>Â¿QuÃ© implicaciones tiene este trabajo para el diseÃ±o de arquitecturas CNN?</b></summary>
<p>Sugiere que la profundidad de la red debe determinarse en funciÃ³n de la complejidad computacional del problema, y no simplemente aumentarse arbitrariamente. Proporciona una base teÃ³rica para decisiones arquitectÃ³nicas que anteriormente se basaban principalmente en la intuiciÃ³n o experimentaciÃ³n.</p>
</details>

<details>
<summary><b>Â¿CÃ³mo se relaciona este trabajo con los teoremas de universalidad de las redes neuronales?</b></summary>
<p>Mientras que los teoremas de universalidad establecen que las redes neuronales pueden aproximar cualquier funciÃ³n continua, nuestro trabajo va mÃ¡s allÃ¡ al relacionar la profundidad con clases especÃ­ficas de complejidad computacional, proporcionando resultados mÃ¡s granulares sobre la capacidad expresiva en relaciÃ³n con la arquitectura.</p>
</details>

<details>
<summary><b>Â¿Se pueden extender estos resultados a otros tipos de redes neuronales?</b></summary>
<p>SÃ­, aunque este trabajo se centra en CNNs, el marco teÃ³rico y las metodologÃ­as experimentales pueden adaptarse a otros tipos de arquitecturas como RNNs, Transformers y GNNs. La relaciÃ³n fundamental entre profundidad y complejidad computacional probablemente se mantenga, con matices especÃ­ficos para cada arquitectura.</p>
</details>

<details>
<summary><b>Â¿CÃ³mo puedo contribuir a este proyecto?</b></summary>
<p>Las contribuciones son bienvenidas en forma de implementaciones adicionales de patrones, optimizaciones de cÃ³digo, extensiones teÃ³ricas o nuevos experimentos. Consulta la secciÃ³n de Contribuciones en este README y revisa los issues abiertos para encontrar Ã¡reas donde puedas contribuir.</p>
</details>

## ğŸ‘¥ Colaboradores

<div align="center">
  <table>
    <tr>
      <td align="center">
        <a href="https://github.com/nrodriguezdiaz">
          <img src="https://miro.medium.com/v2/resize:fit:90/format:webp/1*o2DFooEnpCCmIlzhIXGU1g.png" width="100px;" alt="NicolÃ¡s RodrÃ­guez DÃ­az"/>
          <br />
          <sub><b>NicolÃ¡s RodrÃ­guez DÃ­az</b></sub>
        </a>
        <br />
        <sub>Investigador Principal</sub>
      </td>
      <td align="center">
        <a href="https://github.com/colaborador1">
          <img src="https://miro.medium.com/v2/resize:fit:90/format:webp/1*o2DFooEnpCCmIlzhIXGU1g.png" width="100px;" alt="Colaborador 1"/>
          <br />
          <sub><b>Colaborador 1</b></sub>
        </a>
        <br />
        <sub>Desarrollo de CÃ³digo</sub>
      </td>
      <td align="center">
        <a href="https://github.com/colaborador2">
          <img src="https://miro.medium.com/v2/resize:fit:90/format:webp/1*o2DFooEnpCCmIlzhIXGU1g.png" width="100px;" alt="Colaborador 2"/>
          <br />
          <sub><b>Colaborador 2</b></sub>
        </a>
        <br />
        <sub>AnÃ¡lisis MatemÃ¡tico</sub>
      </td>
    </tr>
  </table>
</div>

## ğŸ™ Agradecimientos

Este trabajo fue posible gracias al apoyo de:

- **Universidad EAFIT** por proporcionar la infraestructura computacional y el entorno acadÃ©mico.
- **Departamento de IngenierÃ­a MatemÃ¡tica** por el soporte teÃ³rico y las discusiones productivas.
- **Revista EIA** por la publicaciÃ³n y difusiÃ³n de los resultados.

Agradecemos especialmente a los revisores anÃ³nimos cuyas sugerencias mejoraron significativamente este trabajo.

## ğŸ“„ Licencia

Este proyecto estÃ¡ licenciado bajo la Licencia MIT - vea el archivo [LICENSE](LICENSE) para mÃ¡s detalles.

## âœ‰ï¸ Contacto

NicolÃ¡s RodrÃ­guez DÃ­az - [nrodrigued@eafit.edu.co](mailto:nrodrigued@eafit.edu.co)

Universidad EAFIT, MedellÃ­n, Colombia

---

<div align="center">

[![Estrellas en GitHub](https://img.shields.io/github/stars/tu-usuario/CNN-Turing-Complexity?style=social)](https://github.com/tu-usuario/CNN-Turing-Complexity/stargazers)
[![Forks en GitHub](https://img.shields.io/github/forks/tu-usuario/CNN-Turing-Complexity?style=social)](https://github.com/tu-usuario/CNN-Turing-Complexity/network/members)

**Si este proyecto te resulta Ãºtil o interesante, considera darle una â­**

[Inicio](#-cnn-turing-complexity-) â€¢ [DocumentaciÃ³n](docs/) â€¢ [Reportar Bug](https://github.com/tu-usuario/CNN-Turing-Complexity/issues) â€¢ [Solicitar FunciÃ³n](https://github.com/tu-usuario/CNN-Turing-Complexity/issues)

</div>
